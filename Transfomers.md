# ğŸ§  TRANSFORMERS Y ATTENTION MECHANISMS

## GuÃ­a de Estudio para Large Language Models

<img width="824" height="690" alt="image" src="https://github.com/user-attachments/assets/95b96ee6-c55e-4df8-b6da-d5b867060c75" />

---

---

## ğŸ¯ **CONCEPTOS CLAVE**

### ğŸ”¥ **Â¿QuÃ© es un Transformer?**

> Arquitectura fundamental de los LLMs modernos (GPT, Claude, BERT)
> 

**Componentes principales:**

- ğŸ§© **Tokenizer** - Convierte texto a nÃºmeros
- ğŸ¯ **Embeddings** - RepresentaciÃ³n vectorial inteligente
- ğŸ“ **Positional Encoding** - InformaciÃ³n del orden
- ğŸ§  **Stack de Transformer Blocks** - Procesamiento principal
- ğŸª **LM Head** - PredicciÃ³n del siguiente token

---

## **ARQUITECTURA COMPLETA DEL TRANSFORMER LLM**

```
ğŸ“ Input Text
    â†“
ğŸ”¢ TOKENIZER
    â†“
ğŸ§© EMBEDDINGS + POSITIONAL ENCODING
    â†“
ğŸ”„ STACK OF TRANSFORMER BLOCKS
   â”‚
   â”œâ”€â”€ ğŸ¯ Self-Attention (relaciones entre palabras)
   â””â”€â”€ ğŸ§  Feed Forward NN (procesamiento complejo)
    â†“
ğŸª LM HEAD (Language Model Head)
    â†“
ğŸ² DECODING STRATEGY
    â†“
ğŸ“¤ Next Output Token
```

---

## ğŸ”¢ Proceso de funcionamiento

## **Paso 1: TokenizaciÃ³n**

```
Texto: "Diego eats pizza"
â†“
Tokens: ["Diego", "eats", "pizza"]

```

## **Paso 2: Word Embeddings**

- **QuÃ© es:** Vector numÃ©rico que representa cada token
- **PropÃ³sito:** Capturar caracterÃ­sticas semÃ¡nticas
- **Ejemplo:**
    
    ```
    "pizza" â†’ [0.2, -0.1, 0.8, 0.5, ...]          
                â†‘     â†‘     â†‘    â†‘        
               comida dulce calor Italia
    ```
    

âŒ **Problema con nÃºmeros fijos:**

`"love" = 5
"hate" = 6
â†’ El modelo pensarÃ­a que "hate" es similar a "love" (nÃºmeros cercanos)`

## **Paso 3: Positional Encoding**

âš ï¸ **Â¿Por quÃ© es crucial?**

| OraciÃ³n A | OraciÃ³n B |
| --- | --- |
| "Diego eats pizza" â†’ **Yum** | "Pizza eats Diego" â†’ **Run** |
- **FunciÃ³n:** Distinguir el orden de las palabras
- **Resultado:** Embeddings + informaciÃ³n posicional

## **Paso 4: Attention**

ğŸ¯ **Determina quÃ© palabras se relacionan entre sÃ­**

**Ejemplo prÃ¡ctico:**

> "La pizza acaba de salir del horno, estaba rica"
> 

**Attention identifica:** "estaba" â†’ se refiere a "pizza" (no a "horno")

---

## ğŸ“ **CONTEXT LENGTH (Longitud del Contexto)**

### **DefiniciÃ³n Simple:**

> CuÃ¡ntas palabras puede "recordar" el modelo en total
> 

### **Ejemplo FÃ¡cil:**

`Pregunta: "I love pizza"    â†’  3 palabras
Respuesta: "yummy"          â†’  1 palabra

Context Length = 4 palabras en total`

### **Â¿Por quÃ© importa?**

- ğŸ§  **Memoria limitada:** El modelo no puede recordar textos infinitos
- ğŸ“ **Conversaciones largas:** Se "olvida" de lo que dijiste al principio
- ğŸ“Š **LÃ­mites actuales:**
    - ChatGPT: ~4,000 palabras
    - Claude: ~100,000 palabras
    - Algunos modelos nuevos: +1,000,000 palabras

---

## ğŸ”„ **PROCESAMIENTO AUTOREGRESIVO**

### **Â¿QuÃ© es Autoregresivo?**

> Cada token generado se agrega al input para generar el siguiente
> 

### **Ejemplo - TraducciÃ³n InglÃ©s â†’ HolandÃ©s:**

`Step 1: "I love llamas" â†’ "ik"
Step 2: "I love llamas ik" â†’ "hou" 
Step 3: "I love llamas ik hou" â†’ "van"
Step 4: "I love llamas ik hou van" â†’ "lama's"

Resultado Final: "ik hou van lama's"`

### **ğŸ”‘ CaracterÃ­sticas Clave:**

- ğŸ¯ **Secuencial:** Una palabra a la vez
- ğŸ”„ **Iterativo:** El output se convierte en input
- ğŸ§  **Contextual:** Cada nueva palabra considera las anteriores

---

## ğŸª **LM HEAD (Language Model Head)**

### **FunciÃ³n Principal:**

> Se encarga de **generar el siguiente token** asignando puntajes a los tokens mÃ¡s probables del vocabulario.
> 

Proceso:
`Transformer Output â†’ LM Head â†’ Probabilidades de Vocabulario`

### **Ejemplo de Output:**

```
TokenId      Token       Probs
0            !           0.01%
1            "           0.03%
.
.
.
1002         Dear        40%
5000         Zyzzyua     1%
```

---

## ğŸ² **ESTRATEGIAS DE DECODIFICACIÃ“N**

### 1ï¸âƒ£ **Greedy Decoding**

`ğŸ¯ Siempre elige el token con mayor probabilidad`

- **ConfiguraciÃ³n:** `temperature = 0`
- **Ventaja:** DeterminÃ­stico y consistente
- **Desventaja:** Puede ser repetitivo

### 2ï¸âƒ£ **Sampling con Temperatura**

`ğŸ² Agrega aleatoriedad controlada`

- **ConfiguraciÃ³n:** `temperature > 0`
- **Proceso:** Selecciona entre los top_p tokens mÃ¡s probables
- **Ventaja:** MÃ¡s creativo y variado
- **Control:** `top_p` determina cuÃ¡ntos tokens considerar

---

## ğŸ§® **TIPOS DE ATTENTION**

### 1ï¸âƒ£ **Self-Attention**

```
ğŸ”„ BIDIRECCIONAL (ve pasado y futuro)

```

**CaracterÃ­sticas:**

- âœ… Analiza **todas** las palabras de la secuencia
- âœ… Incluye la palabra que se estÃ¡ evaluando
- ğŸ¯ **Uso:** Encoders (BERT)

**FÃ³rmula:**

```
Attention(Q,K,V) = SoftMax(QK^T/âˆšd_k) Ã— V

```

### 2ï¸âƒ£ **Masked Self-Attention**

```
â¬…ï¸ UNIDIRECCIONAL (solo ve el pasado)

```

**CaracterÃ­sticas:**

- ğŸš« **NO** ve palabras futuras
- âœ… Solo considera palabra actual y anteriores
- ğŸ¯ **Uso:** Decoders (GPT, Claude, Gemini)

**FÃ³rmula:**

```
Attention(Q,K,V,M) = SoftMax((QK^T/âˆšd_k) + M) Ã— V

```

---

## ğŸ”‘ **COMPONENTES DE LA FÃ“RMULA**

| SÃ­mbolo | Significado | DescripciÃ³n |
| --- | --- | --- |
| **Q** | Query | ğŸ” "Â¿QuÃ© estoy buscando?" |
| **K** | Key | ğŸ—ï¸ Clave/identificador de cada palabra |
| **V** | Value | ğŸ’ Valor asociado con la palabra |
| **M** | Mask | ğŸ­ Bloquea informaciÃ³n futura |
| **d_k** | DimensiÃ³n | ğŸ“ TamaÃ±o del embedding |

### ğŸ§® **CÃ¡lculo de Q, K, V:**

```
Q = Embedding Ã— W_Q  (pesos de Query)
K = Embedding Ã— W_K  (pesos de Key)
V = Embedding Ã— W_V  (pesos de Value)
M = Embedding Ã— W_M  (pesos de Mask)

```

âš¡ **Los pesos W se aprenden durante el entrenamiento y son fijos en inferencia**

---

## ğŸ—ï¸ **ARQUITECTURAS PRINCIPALES**

### ğŸ” **ENCODERS (BERT)**

```
ğŸ“– ComprensiÃ³n bidireccional

```

- **Attention:** Self-Attention
- **PropÃ³sito:** Entender contexto completo
- **Aplicaciones:** ClasificaciÃ³n, Q&A, anÃ¡lisis

### ğŸ—£ï¸ **DECODERS (GPT, Claude, Gemini)**

```
âœï¸ GeneraciÃ³n secuencial

```

- **Attention:** Masked Self-Attention
- **PropÃ³sito:** Generar texto palabra por palabra
- **Aplicaciones:** Chat, escritura, cÃ³digo

---

## ğŸš€ **MULTI-HEAD ATTENTION**

### ğŸ’¡ **Â¿Por quÃ© mÃºltiples cabezas?**

- ğŸ“ **Oraciones largas y complejas** necesitan mÃºltiples perspectivas
- ğŸ¯ Cada "cabeza" se especializa en diferentes tipos de relaciones

### ğŸ”„ **Proceso:**

1. **MÃºltiples capas** de attention en paralelo
2. **ConcatenaciÃ³n** de resultados
3. **ProyecciÃ³n lineal** para ajustar dimensiones

```
[Head1, Head2, Head3, ...] â†’ Concat â†’ Linear â†’ Output

```

âš ï¸ **Importante:** Si el output > embedding size â†’ se agrega capa adicional para reducir el size del output

---

## ğŸ§  **FEED FORWARD NEURAL NETWORK**

> Red neuronal compleja que procesa la informaciÃ³n de attention
> 

### **FunciÃ³n:**

- ğŸ¯ **Transforma** las representaciones de attention
- ğŸ§® **Aplica transformaciones no-lineales**
- ğŸª **Prepara informaciÃ³n** para el LM Head
- ğŸ“Š **MÃºltiples capas** con activaciones (ReLU, GELU)

---

## ğŸ¯ **TIPOS DE EMBEDDINGS GENERADOS**

### 1ï¸âƒ£ **Word Embeddings**

- ğŸ¯ **Clusters de palabras similares**
- Ejemplo: ["gato", "perro", "mascota"] se agrupan

### 2ï¸âƒ£ **Context-Aware Embeddings**

- ğŸ¯ **Clusters de oraciones/documentos**
- Ejemplo: Diferentes significados de "banco" segÃºn contexto
